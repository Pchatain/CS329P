{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c85699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wenxindong/Desktop/Stanford/CS329P/project/riiid-test-answer-prediction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tsfresh import extract_features\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, log_loss, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "%pwd\n",
    "%cd /Users/wenxindong/Desktop/Stanford/CS329P/project/riiid-test-answer-prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe9abc",
   "metadata": {},
   "source": [
    "## load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36ded914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989711, 13) (989711,)\n",
      "(126725, 13) (126725,)\n",
      "(119116, 13) (119116,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answered_correctly_u_avg</th>\n",
       "      <th>elapsed_time_u_avg</th>\n",
       "      <th>explanation_u_avg</th>\n",
       "      <th>answered_correctly_q_avg</th>\n",
       "      <th>elapsed_time_q_avg</th>\n",
       "      <th>answered_correctly_uq_count</th>\n",
       "      <th>part1</th>\n",
       "      <th>part2</th>\n",
       "      <th>part3</th>\n",
       "      <th>part4</th>\n",
       "      <th>part5</th>\n",
       "      <th>part6</th>\n",
       "      <th>part7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "      <td>9.897110e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.243005e-07</td>\n",
       "      <td>1.228034e-07</td>\n",
       "      <td>-3.100500e-07</td>\n",
       "      <td>-8.752863e-07</td>\n",
       "      <td>-6.015538e-07</td>\n",
       "      <td>5.226527e-17</td>\n",
       "      <td>-1.823541e-17</td>\n",
       "      <td>-2.791310e-17</td>\n",
       "      <td>-3.348423e-17</td>\n",
       "      <td>6.892123e-18</td>\n",
       "      <td>8.327982e-17</td>\n",
       "      <td>-6.777255e-18</td>\n",
       "      <td>-9.786815e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.711462e+00</td>\n",
       "      <td>-3.533400e+00</td>\n",
       "      <td>-2.822415e+00</td>\n",
       "      <td>-3.068889e+00</td>\n",
       "      <td>-2.834124e+00</td>\n",
       "      <td>-3.307495e-01</td>\n",
       "      <td>-2.891807e-01</td>\n",
       "      <td>-4.848797e-01</td>\n",
       "      <td>-3.121246e-01</td>\n",
       "      <td>-3.029209e-01</td>\n",
       "      <td>-8.169098e-01</td>\n",
       "      <td>-3.515809e-01</td>\n",
       "      <td>-2.279563e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.652698e-01</td>\n",
       "      <td>-6.408426e-01</td>\n",
       "      <td>7.916185e-02</td>\n",
       "      <td>-5.927076e-01</td>\n",
       "      <td>-5.646055e-01</td>\n",
       "      <td>-3.307495e-01</td>\n",
       "      <td>-2.891807e-01</td>\n",
       "      <td>-4.848797e-01</td>\n",
       "      <td>-3.121246e-01</td>\n",
       "      <td>-3.029209e-01</td>\n",
       "      <td>-8.169098e-01</td>\n",
       "      <td>-3.515809e-01</td>\n",
       "      <td>-2.279563e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.432532e-01</td>\n",
       "      <td>-1.490851e-01</td>\n",
       "      <td>4.294540e-01</td>\n",
       "      <td>1.103226e-01</td>\n",
       "      <td>-1.592306e-01</td>\n",
       "      <td>-3.307495e-01</td>\n",
       "      <td>-2.891807e-01</td>\n",
       "      <td>-4.848797e-01</td>\n",
       "      <td>-3.121246e-01</td>\n",
       "      <td>-3.029209e-01</td>\n",
       "      <td>-8.169098e-01</td>\n",
       "      <td>-3.515809e-01</td>\n",
       "      <td>-2.279563e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.715313e-01</td>\n",
       "      <td>4.410607e-01</td>\n",
       "      <td>5.616751e-01</td>\n",
       "      <td>7.290017e-01</td>\n",
       "      <td>2.662202e-01</td>\n",
       "      <td>-3.307495e-01</td>\n",
       "      <td>-2.891807e-01</td>\n",
       "      <td>-4.848797e-01</td>\n",
       "      <td>-3.121246e-01</td>\n",
       "      <td>-3.029209e-01</td>\n",
       "      <td>1.224124e+00</td>\n",
       "      <td>-3.515809e-01</td>\n",
       "      <td>-2.279563e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.483475e+00</td>\n",
       "      <td>3.789045e+01</td>\n",
       "      <td>6.359219e-01</td>\n",
       "      <td>1.592159e+00</td>\n",
       "      <td>3.072918e+01</td>\n",
       "      <td>2.323279e+01</td>\n",
       "      <td>3.458042e+00</td>\n",
       "      <td>2.062365e+00</td>\n",
       "      <td>3.203846e+00</td>\n",
       "      <td>3.301188e+00</td>\n",
       "      <td>1.224124e+00</td>\n",
       "      <td>2.844293e+00</td>\n",
       "      <td>4.386801e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       answered_correctly_u_avg  elapsed_time_u_avg  explanation_u_avg  \\\n",
       "count              9.897110e+05        9.897110e+05       9.897110e+05   \n",
       "mean               9.243005e-07        1.228034e-07      -3.100500e-07   \n",
       "std                1.000000e+00        1.000000e+00       1.000000e+00   \n",
       "min               -4.711462e+00       -3.533400e+00      -2.822415e+00   \n",
       "25%               -4.652698e-01       -6.408426e-01       7.916185e-02   \n",
       "50%                1.432532e-01       -1.490851e-01       4.294540e-01   \n",
       "75%                6.715313e-01        4.410607e-01       5.616751e-01   \n",
       "max                2.483475e+00        3.789045e+01       6.359219e-01   \n",
       "\n",
       "       answered_correctly_q_avg  elapsed_time_q_avg  \\\n",
       "count              9.897110e+05        9.897110e+05   \n",
       "mean              -8.752863e-07       -6.015538e-07   \n",
       "std                1.000000e+00        1.000000e+00   \n",
       "min               -3.068889e+00       -2.834124e+00   \n",
       "25%               -5.927076e-01       -5.646055e-01   \n",
       "50%                1.103226e-01       -1.592306e-01   \n",
       "75%                7.290017e-01        2.662202e-01   \n",
       "max                1.592159e+00        3.072918e+01   \n",
       "\n",
       "       answered_correctly_uq_count         part1         part2         part3  \\\n",
       "count                 9.897110e+05  9.897110e+05  9.897110e+05  9.897110e+05   \n",
       "mean                  5.226527e-17 -1.823541e-17 -2.791310e-17 -3.348423e-17   \n",
       "std                   1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min                  -3.307495e-01 -2.891807e-01 -4.848797e-01 -3.121246e-01   \n",
       "25%                  -3.307495e-01 -2.891807e-01 -4.848797e-01 -3.121246e-01   \n",
       "50%                  -3.307495e-01 -2.891807e-01 -4.848797e-01 -3.121246e-01   \n",
       "75%                  -3.307495e-01 -2.891807e-01 -4.848797e-01 -3.121246e-01   \n",
       "max                   2.323279e+01  3.458042e+00  2.062365e+00  3.203846e+00   \n",
       "\n",
       "              part4         part5         part6         part7  \n",
       "count  9.897110e+05  9.897110e+05  9.897110e+05  9.897110e+05  \n",
       "mean   6.892123e-18  8.327982e-17 -6.777255e-18 -9.786815e-17  \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "min   -3.029209e-01 -8.169098e-01 -3.515809e-01 -2.279563e-01  \n",
       "25%   -3.029209e-01 -8.169098e-01 -3.515809e-01 -2.279563e-01  \n",
       "50%   -3.029209e-01 -8.169098e-01 -3.515809e-01 -2.279563e-01  \n",
       "75%   -3.029209e-01  1.224124e+00 -3.515809e-01 -2.279563e-01  \n",
       "max    3.301188e+00  1.224124e+00  2.844293e+00  4.386801e+00  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pickle = 'train_39360_users_preprocessed.pickle'  #about one tenth of the training dataset\n",
    "valid_pickle = 'valid_4920_users_preprocessed.pickle'  \n",
    "test_pickle = \"test_4920_users_preprocessed.pickle\"\n",
    "question_file = 'questions.csv'\n",
    "\n",
    "# Read data\n",
    "train = pd.read_pickle(train_pickle)\n",
    "valid = pd.read_pickle(valid_pickle)\n",
    "test = pd.read_pickle(test_pickle)\n",
    "\n",
    "questions_df = pd.read_csv(question_file)\n",
    "train = train.fillna(0)\n",
    "valid = valid.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "#subsample\n",
    "train = train[:len(train)//10]\n",
    "valid = valid[:len(valid)//10]\n",
    "test = test[:len(test)//10]\n",
    "\n",
    "for i in range(1,8):\n",
    "  train['part'+str(i)] = (train[\"part\"]==i)\n",
    "  valid['part'+str(i)] = (valid[\"part\"]==i)\n",
    "  test['part'+str(i)] = (test[\"part\"]==i)\n",
    "\n",
    "#get labels\n",
    "TARGET = 'answered_correctly'\n",
    "train_x = train.drop([TARGET], axis = 1)\n",
    "valid_x = valid.drop([TARGET], axis = 1)\n",
    "test_x = test.drop([TARGET], axis = 1)\n",
    "\n",
    "train_y = train[TARGET]\n",
    "valid_y = valid[TARGET]\n",
    "test_y = test[TARGET]\n",
    "\n",
    "#start with 4 features only\n",
    "column_features = [\"explanation_u_avg\", \"timestamp_u_incorrect_recency\", \"explanation_q_avg\", \"prior_question_had_explanation\",\"part\", \"timestamp_u_recency_3\", \"timestamp_u_recency_2\", \"timestamp_u_recency_1\",\"prior_question_elapsed_time\", \"answered_correctly_u_avg\",\"elapsed_time_u_avg\",  \"answered_correctly_uq_count\", \"elapsed_time_q_avg\", \"answered_correctly_q_avg\"]\n",
    "train_x = train_x.drop([ \"timestamp_u_incorrect_recency\", \"explanation_q_avg\", \"prior_question_had_explanation\",\"part\", \"timestamp_u_recency_3\", \"timestamp_u_recency_2\", \"timestamp_u_recency_1\", \"prior_question_elapsed_time\"], axis= 1)\n",
    "valid_x = valid_x.drop([ \"timestamp_u_incorrect_recency\", \"explanation_q_avg\", \"prior_question_had_explanation\",\"part\", \"timestamp_u_recency_3\", \"timestamp_u_recency_2\", \"timestamp_u_recency_1\", \"prior_question_elapsed_time\"], axis=1)\n",
    "test_x = test_x.drop([ \"timestamp_u_incorrect_recency\", \"explanation_q_avg\", \"prior_question_had_explanation\",\"part\", \"timestamp_u_recency_3\", \"timestamp_u_recency_2\", \"timestamp_u_recency_1\", \"prior_question_elapsed_time\"], axis=1)\n",
    "\n",
    "#binary classification acc = 0.97\n",
    "mean = train_x.mean()\n",
    "std = train_x.std()\n",
    "train_x = (train_x -train_x.mean())/train_x.std()\n",
    "valid_x = (valid_x -mean)/std\n",
    "test_x = (test_x -mean)/std\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "\n",
    "train_x.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2344831d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989711, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#soft labels \n",
    "autogl_predict_proba = pd.read_pickle(\"autogl_soft_label.pickle\")\n",
    "autogl_predict_proba = np.array(autogl_predict_proba[[1]])\n",
    "autogl_predict_proba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858fba2",
   "metadata": {},
   "source": [
    "## Soft decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e464d7fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training features (989711, 13)\n",
      "training labels (989711, 1)\n",
      "Soft: sparse= False, 150 epochs, 13 input dimension, 0.01 learing rate, 4 depth\n",
      "EPOCH 0\n",
      "training acc 0.4785952666990667, validation acc 0.4802682974945749 test acc 0.48128714866180866\n",
      "validation auc 0.46935931146228915 test auc 0.46674931254340996\n",
      "EPOCH 0\n",
      "training acc 0.49321367550729456, validation acc 0.49235746695600713 test acc 0.4979012055475335\n",
      "validation auc 0.4973710002532302 test auc 0.4920780385760205\n",
      "EPOCH 10\n",
      "training acc 0.6766005429867911, validation acc 0.6712250937068456 test acc 0.6750394573357064\n",
      "validation auc 0.6343275698770849 test auc 0.6310134392753212\n",
      "EPOCH 20\n",
      "training acc 0.6903873959165857, validation acc 0.6829591635431052 test acc 0.6889166862554149\n",
      "validation auc 0.6805825624587978 test auc 0.6820764767257763\n",
      "EPOCH 30\n",
      "training acc 0.6970085206691651, validation acc 0.6911422371276386 test acc 0.6972195171093724\n",
      "validation auc 0.7062984983047225 test auc 0.7098176212412375\n",
      "EPOCH 40\n",
      "training acc 0.7033133914849891, validation acc 0.6992779640954824 test acc 0.7056230900970483\n",
      "validation auc 0.7226288594398076 test auc 0.7272842821727467\n",
      "EPOCH 50\n",
      "training acc 0.7074954203802928, validation acc 0.7074294732688893 test acc 0.7133214681486953\n",
      "validation auc 0.730656734866499 test auc 0.7355330876109054\n",
      "EPOCH 60\n",
      "training acc 0.7103639345223, validation acc 0.7120220950877885 test acc 0.717796097921354\n",
      "validation auc 0.7361017828907088 test auc 0.7412868838536644\n",
      "EPOCH 70\n",
      "training acc 0.712782822460294, validation acc 0.7150443874531466 test acc 0.7207260149769972\n",
      "validation auc 0.739568913612497 test auc 0.7445045059401132\n",
      "EPOCH 80\n",
      "training acc 0.7156493158103729, validation acc 0.7172381140264352 test acc 0.7230346888747103\n",
      "validation auc 0.7416295039693896 test auc 0.746300730458199\n",
      "EPOCH 90\n",
      "training acc 0.7169294874968551, validation acc 0.7201262576445058 test acc 0.7258890493300648\n",
      "validation auc 0.743445239399907 test auc 0.7478099299166733\n",
      "EPOCH 100\n",
      "training acc 0.7178994676223666, validation acc 0.7218149536397711 test acc 0.7270895597568756\n",
      "validation auc 0.7448270076736231 test auc 0.749038182130587\n",
      "EPOCH 110\n",
      "training acc 0.7185491522272663, validation acc 0.723085421187611 test acc 0.7282313039390175\n",
      "validation auc 0.7458910347324379 test auc 0.7499158502194011\n",
      "EPOCH 120\n",
      "training acc 0.7189432066532554, validation acc 0.7234563030183468 test acc 0.7290036602975251\n",
      "validation auc 0.7467739170198762 test auc 0.750757945237756\n",
      "EPOCH 130\n",
      "training acc 0.7192907828648969, validation acc 0.7242217399881633 test acc 0.7295493468551664\n",
      "validation auc 0.7474816257235994 test auc 0.7514453096476639\n",
      "EPOCH 140\n",
      "training acc 0.719618151157257, validation acc 0.7243243243243244 test acc 0.7296500889888848\n",
      "validation auc 0.7478843826391287 test auc 0.7517950308128882\n",
      "training acc 0.7197687001559041, validation acc 0.7245610574077728 test acc 0.7298515732563216\n",
      "validation auc 0.7482691585633059 test auc 0.7520948437322372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SoftDecisionTree(\n",
       "  (level1): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=1, bias=True)\n",
       "  )\n",
       "  (level2): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=2, bias=True)\n",
       "  )\n",
       "  (level3): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=4, bias=True)\n",
       "  )\n",
       "  (level4): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SoftDecisionTreeNode(nn.Module):\n",
    "    def __init__(self, in_features=10):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, 1, bias = True)\n",
    "    def forward(self,x):\n",
    "        temp = self.linear(x)\n",
    "        output = nn.Sigmoid()(temp)\n",
    "        return output\n",
    "\n",
    "class SoftDecisionTreeLevel(nn.Module):\n",
    "    def __init__(self, in_features=10, depth = 1, weight = None, bias = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_nodes = 2**(depth-1)\n",
    "        self.in_features = in_features\n",
    "        self.weight = weight\n",
    "        if self.weight !=None:\n",
    "            self.weight = weight\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.linear = torch.nn.Linear(in_features, self.num_nodes, bias = True)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        temp = None\n",
    "        if self.weight !=None:\n",
    "            temp = x@ self.weight.T + self.bias\n",
    "        else:\n",
    "            temp = self.linear(x)\n",
    "        output = nn.Sigmoid()(temp)\n",
    "        return output\n",
    "    \n",
    "class SoftDecisionTreeSparseLevel(nn.Module):\n",
    "    #one feature per node \n",
    "    #option 1: specify feature per node (to determine feature order)\n",
    "    #option 2: let feature be i for level i (baseline, same featureo on each level)\n",
    "    def __init__(self, in_features=10, freeze=False, depth = 1, feature = 0):\n",
    "        \n",
    "        #feature could be an int or arr\n",
    "        super().__init__()\n",
    "        self.num_nodes = 2**(depth-1)\n",
    "        self.in_features = in_features\n",
    "        self.feature = feature\n",
    "        self.freeze = freeze\n",
    "        if self.freeze: #option 1\n",
    "            for i in range(self.num_nodes):\n",
    "                setattr(self, \"linear\" + str(i), SoftDecisionTreeNode(in_features= 1))\n",
    "        else:\n",
    "            self.linear = torch.nn.Linear(1, self.num_nodes, bias = True) \n",
    "    def forward(self,x):\n",
    "        temp = torch.zeros((x.shape[0], self.num_nodes))\n",
    "        \n",
    "        if self.freeze:\n",
    "            for i in range(self.num_nodes):\n",
    "                node = getattr(self, \"linear\"+str(i))(x[:, [self.feature[i]]])\n",
    "                temp[:, [i]] = node               \n",
    "        else:\n",
    "            temp = self.linear(x[:, [self.feature]])\n",
    "        output = nn.Sigmoid()(temp)\n",
    "        return output\n",
    "    \n",
    "class SoftDecisionTree(nn.Module):\n",
    "    def __init__(self, in_features=10,depth=2, sparse = True, weights = None, biases = None, freeze= False, features = None):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.depth = depth\n",
    "        self.freeze = freeze\n",
    "        self.sparse = sparse\n",
    "        for d in range(1, self.depth+1):\n",
    "            if self.sparse:\n",
    "                f = d-1\n",
    "                if self.freeze:\n",
    "                    f = features[d-1]\n",
    "                setattr(self, \"level\" + str(d), SoftDecisionTreeSparseLevel(in_features= self.in_features, depth = d, feature = f, freeze=freeze))\n",
    "            else:\n",
    "                if weights ==None:\n",
    "                    setattr(self, \"level\" + str(d), SoftDecisionTreeLevel(in_features= self.in_features, depth = d))\n",
    "                else:\n",
    "                    setattr(self, \"level\" + str(d), SoftDecisionTreeLevel(in_features= self.in_features, depth = d, weight = weights[d-1], bias = biases[d-1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = []\n",
    "        for d in range(1, self.depth+1):\n",
    "            probs.append(getattr(self, \"level\"+str(d))(x))\n",
    "        temp = probs[-1]\n",
    "        for i in range(self.depth-2, -1, -1):\n",
    "            temp = probs[i]*(temp.reshape(-1,2)[:,1].reshape(-1, 2**i)) + (1-probs[i])*(temp.reshape(-1,2)[:,0].reshape(-1, 2**i))\n",
    "        prediction = temp\n",
    "        return prediction\n",
    "\n",
    "#dummy dataset\n",
    "# x_train = np.array([[0,1],[1,1],[2,2],[3,3],[4,4],[5,5],[6,6],[7,7], [8,6], [9,9]], dtype=np.float32)\n",
    "# y_train = np.array([0,0,1,1,0,0,0,0, 1, 1], dtype=np.float32).reshape(-1, 1)\n",
    "# y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "#riiid dataset\n",
    "x_train = train_x.to_numpy(dtype=np.float32)\n",
    "x_valid = valid_x.to_numpy(dtype=np.float32)\n",
    "x_test = test_x.to_numpy(dtype=np.float32)\n",
    "\n",
    "y_train = train_y.to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "y_valid = valid_y.to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "y_test = test_y.to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(\"training features\", x_train.shape)\n",
    "print(\"training labels\", y_train.shape)\n",
    "\n",
    "def evaluate(model, visualize = False, inputDim = 13):\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "        else:\n",
    "            predicted_train = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "            predicted_valid = model(Variable(torch.from_numpy(x_valid))).data.numpy()\n",
    "            predicted_test = model(Variable(torch.from_numpy(x_test))).data.numpy()\n",
    "            \n",
    "            final_prediction_train = predicted_train>0.5\n",
    "            final_prediction_valid = predicted_valid>0.5\n",
    "            final_prediction_test = predicted_test>0.5\n",
    "            accuracy_train = accuracy_score(y_train, final_prediction_train)\n",
    "            accuracy_valid = accuracy_score(y_valid, final_prediction_valid)\n",
    "            accuracy_test = accuracy_score(y_test, final_prediction_test)\n",
    "            \n",
    "            auc_valid = roc_auc_score(y_valid, predicted_valid)\n",
    "            auc_test = roc_auc_score(y_test, predicted_test)\n",
    "            \n",
    "            print(\"training acc {}, validation acc {} test acc {}\".format(accuracy_train,accuracy_valid, accuracy_test))\n",
    "            print(\"validation auc {} test auc {}\".format(auc_valid, auc_test))\n",
    "    if visualize:\n",
    "        for i in range(inputDim):\n",
    "            color = ['red' if x==1 else 'black' for x in y_train]\n",
    "            plt.scatter(x_valid[:, i], predicted_valid, c= color, alpha=0.5)\n",
    "            plt.legend(loc='best')\n",
    "            plt.title(\"feature\"+str(i))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def train_softtree(depth = 4, sparse=True, epochs = 100, weights = None, \n",
    "                   biases = None, visualize = True, verbose=False, freeze=False, features = None, lr=0.01, soft_labels = False):\n",
    "    \n",
    "    #model\n",
    "    inputDim = x_train.shape[1]       \n",
    "    outputDim = 1       \n",
    "    learningRate = lr\n",
    "    model = SoftDecisionTree(in_features = inputDim, depth=depth, sparse=sparse, freeze = freeze,\n",
    "                             weights = weights, biases = biases,features = features)\n",
    "#     if verbose: print(model)\n",
    "    print(\"Soft: sparse= {}, {} epochs, {} input dimension, {} learing rate, {} depth\".format(sparse, epochs, inputDim, learningRate, depth))\n",
    "\n",
    "    #train model\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    criterion = torch.nn.BCELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    train_loss =[]\n",
    "    valid_loss = []\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        if soft_labels:\n",
    "            labels = Variable(torch.from_numpy(autogl_predict_proba).cuda())\n",
    "        else:\n",
    "            labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "        inputs_val = Variable(torch.from_numpy(x_valid).cuda())\n",
    "        labels_val = Variable(torch.from_numpy(y_valid).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        if soft_labels:\n",
    "            labels = Variable(torch.from_numpy(autogl_predict_proba))\n",
    "        else:\n",
    "            labels = Variable(torch.from_numpy(y_train))\n",
    "        inputs_val = Variable(torch.from_numpy(x_valid))\n",
    "        labels_val = Variable(torch.from_numpy(y_valid))\n",
    "    print(\"EPOCH 0\")\n",
    "    evaluate(model)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = - torch.sum((torch.log(outputs)*labels + torch.log(1-outputs)*(1-labels))) / outputs.shape[0]\n",
    "        #criterion(outputs, labels)\n",
    "        \n",
    "        l1_lambda = 0.001\n",
    "        reg = l1_lambda * sum(p.abs().sum() for name, p in model.named_parameters() if \"weight\" in name)\n",
    "        loss.backward()\n",
    "        if not sparse:\n",
    "            reg.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%10==0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs_valid = model(inputs_val)\n",
    "                loss_valid = criterion(outputs_valid, labels_val)\n",
    "                valid_loss.append(loss_valid)\n",
    "                train_loss.append(loss)\n",
    "            if verbose: \n",
    "                print(\"EPOCH {}\".format(epoch))\n",
    "                evaluate(model, visualize = visualize, inputDim = inputDim)\n",
    "    evaluate(model, visualize = visualize, inputDim = inputDim)\n",
    "        #inference\n",
    "    if visualize:\n",
    "        plt.plot(valid_loss, label = \"validation loss\")\n",
    "        plt.plot(train_loss, label = \"training loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    return model\n",
    "#compare model performance \n",
    "\n",
    "# #normal decision tree\n",
    "# NormalDecisionTree(max_depth=1)\n",
    "# NormalDecisionTree(max_depth=2)\n",
    "# NormalDecisionTree(max_depth=3)\n",
    "# NormalDecisionTree(max_depth=4)\n",
    "# NormalDecisionTree(max_depth=5)\n",
    "# NormalDecisionTree(max_depth=6)\n",
    "# print()\n",
    "# #soft decision tree with sparse nodes:\n",
    "# train_softtree(depth = 1, sparse=True, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 2, sparse=True, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 3, sparse=True, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 4, sparse=True, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 5, sparse=True, epochs = 100, visualize = False, verbose=False)\n",
    "# deterministic_sparse_model = train_softtree(depth = 6, sparse=True, epochs = 100, visualize = False, verbose=False)\n",
    "# print()\n",
    "\n",
    "# #soft decision tree with non-sparse nodes:\n",
    "# train_softtree(depth = 1, sparse=False, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 2, sparse=False, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 3, sparse=False, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 4, sparse=False, epochs = 100, visualize = False, verbose=False)\n",
    "# train_softtree(depth = 5, sparse=False, epochs = 100, visualize = False, verbose=False)\n",
    "\n",
    "        \n",
    "train_softtree(depth = 4, sparse=False, epochs = 150, visualize = False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19fe2570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft: sparse= False, 150 epochs, 13 input dimension, 0.01 learing rate, 4 depth\n",
      "EPOCH 0\n",
      "training acc 0.41145647567825355, validation acc 0.4206667981850464 test acc 0.4306558312905067\n",
      "validation auc 0.3891836278906889 test auc 0.3921958032580744\n",
      "EPOCH 0\n",
      "training acc 0.4591613107260604, validation acc 0.46865259420003946 test acc 0.4799691057456597\n",
      "validation auc 0.4082335315009417 test auc 0.41144001762657506\n",
      "EPOCH 10\n",
      "training acc 0.6754951698020938, validation acc 0.6693549023476031 test acc 0.6751150139359952\n",
      "validation auc 0.6367146206922618 test auc 0.6319229949867211\n",
      "EPOCH 20\n",
      "training acc 0.6899741439672793, validation acc 0.68621029788913 test acc 0.6901927532825145\n",
      "validation auc 0.6739626664750514 test auc 0.6724381181698622\n",
      "EPOCH 30\n",
      "training acc 0.6964426989292833, validation acc 0.6925073979088577 test acc 0.6981597770240774\n",
      "validation auc 0.7031948883229185 test auc 0.7036959935081881\n",
      "EPOCH 40\n",
      "training acc 0.7024323262043162, validation acc 0.7007220359045176 test acc 0.706831995701669\n",
      "validation auc 0.7225764162573104 test auc 0.7250778615021545\n",
      "EPOCH 50\n",
      "training acc 0.7066840724211412, validation acc 0.7093233379364766 test acc 0.7156469324020283\n",
      "validation auc 0.7326886192045525 test auc 0.7365387147248942\n",
      "EPOCH 60\n",
      "training acc 0.7094464949869205, validation acc 0.7151232984809627 test acc 0.72180059773666\n",
      "validation auc 0.7381321860429559 test auc 0.7424657658724685\n",
      "EPOCH 70\n",
      "training acc 0.7116461270007103, validation acc 0.7184375616492404 test acc 0.7247808858591624\n",
      "validation auc 0.741322253041724 test auc 0.7457924856265015\n",
      "EPOCH 80\n",
      "training acc 0.7137982704041888, validation acc 0.7194712961136319 test acc 0.7258974445078746\n",
      "validation auc 0.7425981615757911 test auc 0.7466354543177782\n",
      "EPOCH 90\n",
      "training acc 0.715922122720673, validation acc 0.7205602682974945 test acc 0.7262416467980792\n",
      "validation auc 0.7435697965223433 test auc 0.7480372394960263\n",
      "EPOCH 100\n",
      "training acc 0.7168749261147951, validation acc 0.721286249753403 test acc 0.7265858490882837\n",
      "validation auc 0.7441152373200413 test auc 0.7486500177578338\n",
      "EPOCH 110\n",
      "training acc 0.7172922196479579, validation acc 0.7215782205563227 test acc 0.7269972128009671\n",
      "validation auc 0.7446206125173126 test auc 0.7491523027778875\n",
      "EPOCH 120\n",
      "training acc 0.717755991395468, validation acc 0.7219648845926219 test acc 0.7272490681352631\n",
      "validation auc 0.7458618350316276 test auc 0.7503544052449118\n",
      "EPOCH 130\n",
      "training acc 0.7180459750371573, validation acc 0.7223199842177944 test acc 0.7277695691594748\n",
      "validation auc 0.746508974175493 test auc 0.7509746835986078\n",
      "EPOCH 140\n",
      "training acc 0.7183612185779485, validation acc 0.7226593016374038 test acc 0.728130561805299\n",
      "validation auc 0.7471586332097939 test auc 0.7515617742135887\n",
      "training acc 0.7185764329182963, validation acc 0.7229986190570132 test acc 0.7284831592733134\n",
      "validation auc 0.7476600550342952 test auc 0.7518818936325251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SoftDecisionTree(\n",
       "  (level1): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=1, bias=True)\n",
       "  )\n",
       "  (level2): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=2, bias=True)\n",
       "  )\n",
       "  (level3): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=4, bias=True)\n",
       "  )\n",
       "  (level4): SoftDecisionTreeLevel(\n",
       "    (linear): Linear(in_features=13, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_softtree(depth = 4, sparse=False, epochs = 150, visualize = False, verbose=True, soft_labels = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34aaeb",
   "metadata": {},
   "source": [
    "## Extract weight from softdecision tree, zero out weights except for one feature per node, and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab27513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft: sparse= False, 100 epochs, 13 input dimension, 0.01 learing rate, 4 depth\n",
      "before finetuning\n",
      "training acc 0.5431312777164243, validation acc 0.5435786151114618 test acc 0.5652809026495181\n",
      "validation auc 0.38339563289247264 test auc 0.4042055035986394\n",
      "epoch 0, training loss 0.6996117234230042, validation loss 0.6923410296440125\n",
      "epoch 10, training loss 0.6328803896903992, validation loss 0.6322885155677795\n",
      "epoch 20, training loss 0.6010537147521973, validation loss 0.6031866073608398\n",
      "epoch 30, training loss 0.5854806303977966, validation loss 0.588189959526062\n",
      "epoch 40, training loss 0.5766927003860474, validation loss 0.5785672664642334\n",
      "epoch 50, training loss 0.571621835231781, validation loss 0.5722051858901978\n",
      "epoch 60, training loss 0.5677770972251892, validation loss 0.5672820806503296\n",
      "epoch 70, training loss 0.5645943880081177, validation loss 0.5633003115653992\n",
      "epoch 80, training loss 0.5621489882469177, validation loss 0.560206413269043\n",
      "epoch 90, training loss 0.5601969957351685, validation loss 0.5574272871017456\n",
      "training acc 0.7172366478699338, validation acc 0.7205997238114027 test acc 0.7262332516202693\n",
      "validation auc 0.7466951601266693 test auc 0.7512258939845782\n",
      "tree with one feature in each node retrained with pretrained initial weight:\n",
      "Soft: sparse= True, 1 epochs, 13 input dimension, 0.0001 learing rate, 4 depth\n",
      "before finetuning\n",
      "training acc 0.67302677246186, validation acc 0.6656460840402446 test acc 0.6720759595688237\n",
      "validation auc 0.27222047527724985 test auc 0.26654929405013345\n",
      "epoch 0, training loss 0.6470118165016174, validation loss 0.6503780484199524\n",
      "training acc 0.67302677246186, validation acc 0.6656460840402446 test acc 0.6720759595688237\n",
      "validation auc 0.27223630597515386 test auc 0.26656419781649215\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def extract_weights_features(softtree_model):\n",
    "    #for tree models that have all features in all nodes\n",
    "    weights = []\n",
    "    biases = []\n",
    "    features = []\n",
    "    for i, (name, param) in enumerate(softtree_model.named_parameters()):    \n",
    "        if i%2 == 1:\n",
    "            biases.append(param.data)\n",
    "        else:\n",
    "            weight = torch.zeros_like(param.data)\n",
    "            where = torch.argmax(torch.abs(param.data), dim=1)\n",
    "            features.append(where.cpu().detach().numpy())\n",
    "            weight[torch.arange(where.shape[0]), where] = param.data[torch.arange(where.shape[0]), where] \n",
    "            weights.append(weight)\n",
    "    return weights, biases, features\n",
    "        \n",
    "full_model = train_softtree(depth = 4, sparse=False, epochs = 100, visualize = False, verbose=True)\n",
    "\n",
    "weights, biases, features = extract_weights_features(full_model)\n",
    "# print(\"tree with one feature in each node using pretrained weight only:\")\n",
    "# sparse_model = SoftDecisionTree(in_features = 13, depth=4, sparse=False, weights = weights, biases = biases)\n",
    "# evaluate(sparse_model)\n",
    "\n",
    "print(\"tree with one feature in each node retrained with pretrained initial weight:\")\n",
    "sparse_model_retrained = train_softtree(depth = 4, sparse=True, epochs = 1,  weights = weights, biases = biases, \n",
    "                                       freeze = True,  features = features, visualize = False, verbose=True, lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d3cdb",
   "metadata": {},
   "source": [
    "## Working on visualizing the tree (wenxin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "0143d7cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 1 feature elapsed_time_u_avg threshold -1.0497232675552368\n",
      "level 2 feature explanation_u_avg threshold 1.270133137702942\n",
      "level 2 feature explanation_u_avg threshold -1.0496875047683716\n",
      "Index(['answered_correctly_u_avg', 'elapsed_time_u_avg', 'explanation_u_avg',\n",
      "       'answered_correctly_q_avg', 'elapsed_time_q_avg',\n",
      "       'answered_correctly_uq_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def inverse_sigmoid(y):\n",
    "    return np.log(y/(1-y))\n",
    "train_x.columns\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_disjoint_linear_layers(model, features):\n",
    "    \n",
    "    #visualize tree model\n",
    "    #that had one non-zero weight per node\n",
    "    #that trained #children linear layer per level\n",
    "    weight = None\n",
    "    thresholds = []\n",
    "    \n",
    "    feature_names = train_x.columns\n",
    "    feature_names = [feature_names[feature_idx]  for level in features for feature_idx in level]\n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "        if i%2== 1:\n",
    "            bias = torch.flatten(param.data)\n",
    "            threshold = (inverse_sigmoid(0.5) + bias)/weight\n",
    "            thresholds.append(threshold)\n",
    "            print(\"{} threshold {}\".format(feature_names[i//2], threshold))\n",
    "        else:\n",
    "            weight = torch.flatten(param.data)\n",
    "\n",
    "# visualize_disjoint_linear_layers(sparse_model_retrained, features)\n",
    "\n",
    "\n",
    "@torch.no_grad()  \n",
    "def visualize_single_linear_layer(model, features=None):\n",
    "\n",
    "    #visualize tree model\n",
    "    #that had one non-zero weight per node\n",
    "    #that trained one big linear layer per level\n",
    "    \n",
    "    feature_names = train_x.columns\n",
    "#     feature_names = [feature_names[feature_idx]  for level in features for feature_idx in level]\n",
    "    weight = None\n",
    "    level = 0\n",
    "#     print(model.level1.weight)\n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#         print(name, param)\n",
    "        if i%2== 1:\n",
    "            bias = torch.flatten(param.data)\n",
    "            threshold = (inverse_sigmoid(0.5) + bias)/weight\n",
    "            for node_threshold in threshold:\n",
    "                print(\"level {} feature {} threshold {}\".format(level, feature_names[level], node_threshold))\n",
    "        else:\n",
    "            weight = torch.flatten(param.data)\n",
    "            level+=1\n",
    "    \n",
    "    print(feature_names)\n",
    "visualize_single_linear_layer(deterministic_sparse_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e692ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
